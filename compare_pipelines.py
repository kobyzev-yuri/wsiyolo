#!/usr/bin/env python3
"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∏ –Ω–æ–≤–æ–≥–æ WSI YOLO Pipeline.
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
"""

import os
import json
import time
import numpy as np
from pathlib import Path
import sys
from typing import Dict, List, Any

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent / "src"))

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ pipeline
from wsi_yolo_pipeline import WSIYOLOPipeline

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –Ω–æ–≤–æ–≥–æ pipeline
from improved_wsi_yolo_pipeline import ImprovedWSIYOLOPipeline

def load_predictions(file_path: str) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
        return {}

def compare_predictions(old_predictions: List[Dict], new_predictions: List[Dict]) -> Dict[str, Any]:
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤—É—Ö pipeline"""
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
    old_count = len(old_predictions)
    new_count = len(new_predictions)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
    old_classes = {}
    new_classes = {}
    
    for pred in old_predictions:
        class_name = pred.get('class_name', 'unknown')
        old_classes[class_name] = old_classes.get(class_name, 0) + 1
    
    for pred in new_predictions:
        class_name = pred.get('class_name', 'unknown')
        new_classes[class_name] = new_classes.get(class_name, 0) + 1
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–∏–≥–æ–Ω–æ–≤
    old_polygon_stats = analyze_polygon_statistics(old_predictions)
    new_polygon_stats = analyze_polygon_statistics(new_predictions)
    
    return {
        'count_comparison': {
            'old_count': old_count,
            'new_count': new_count,
            'difference': new_count - old_count,
            'change_percent': (new_count - old_count) / old_count * 100 if old_count > 0 else 0
        },
        'class_comparison': {
            'old_classes': old_classes,
            'new_classes': new_classes
        },
        'polygon_analysis': {
            'old_stats': old_polygon_stats,
            'new_stats': new_polygon_stats
        }
    }

def analyze_polygon_statistics(predictions: List[Dict]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–ª–∏–≥–æ–Ω–æ–≤"""
    if not predictions:
        return {}
    
    polygon_counts = []
    areas = []
    perimeters = []
    simplification_metrics = []
    
    for pred in predictions:
        if pred.get('polygon'):
            polygon_counts.append(len(pred['polygon']))
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–ª–æ—â–∞–¥—å –∏ –ø–µ—Ä–∏–º–µ—Ç—Ä (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
            if len(pred['polygon']) >= 3:
                # –ü—Ä–æ—Å—Ç–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–∏ —á–µ—Ä–µ–∑ shoelace formula
                coords = [(p['x'], p['y']) for p in pred['polygon']]
                area = calculate_polygon_area(coords)
                perimeter = calculate_polygon_perimeter(coords)
                
                areas.append(area)
                perimeters.append(perimeter)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —É–ø—Ä–æ—â–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'simplification_metrics' in pred:
            metrics = pred['simplification_metrics']
            simplification_metrics.append(metrics)
    
    stats = {
        'total_predictions': len(predictions),
        'predictions_with_polygons': len([p for p in predictions if p.get('polygon')]),
        'avg_polygon_points': np.mean(polygon_counts) if polygon_counts else 0,
        'max_polygon_points': max(polygon_counts) if polygon_counts else 0,
        'min_polygon_points': min(polygon_counts) if polygon_counts else 0
    }
    
    if areas:
        stats.update({
            'avg_area': np.mean(areas),
            'total_area': np.sum(areas),
            'avg_perimeter': np.mean(perimeters)
        })
    
    if simplification_metrics:
        area_preserved = [m.get('area_preserved', 1.0) for m in simplification_metrics]
        stats.update({
            'avg_area_preserved': np.mean(area_preserved),
            'min_area_preserved': np.min(area_preserved),
            'simplification_applied': len(simplification_metrics)
        })
    
    return stats

def calculate_polygon_area(coords: List[tuple]) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–ª–æ—â–∞–¥—å –ø–æ–ª–∏–≥–æ–Ω–∞ –ø–æ —Ñ–æ—Ä–º—É–ª–µ —à–Ω—É—Ä–∫–∞"""
    if len(coords) < 3:
        return 0.0
    
    n = len(coords)
    area = 0.0
    for i in range(n):
        j = (i + 1) % n
        area += coords[i][0] * coords[j][1]
        area -= coords[j][0] * coords[i][1]
    return abs(area) / 2.0

def calculate_polygon_perimeter(coords: List[tuple]) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–µ—Ä–∏–º–µ—Ç—Ä –ø–æ–ª–∏–≥–æ–Ω–∞"""
    if len(coords) < 2:
        return 0.0
    
    perimeter = 0.0
    for i in range(len(coords)):
        j = (i + 1) % len(coords)
        dx = coords[j][0] - coords[i][0]
        dy = coords[j][1] - coords[i][1]
        perimeter += np.sqrt(dx*dx + dy*dy)
    
    return perimeter

def compare_performance(old_stats: Dict, new_stats: Dict) -> Dict[str, Any]:
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å pipeline"""
    
    old_time = old_stats.get('processing_time', 0)
    new_time = new_stats.get('processing_time', 0)
    
    old_patches = old_stats.get('total_patches', 0)
    new_patches = new_stats.get('total_patches', 0)
    
    return {
        'time_comparison': {
            'old_time': old_time,
            'new_time': new_time,
            'speedup': old_time / new_time if new_time > 0 else 0,
            'time_saved': old_time - new_time
        },
        'throughput_comparison': {
            'old_throughput': old_patches / old_time if old_time > 0 else 0,
            'new_throughput': new_patches / new_time if new_time > 0 else 0,
            'throughput_improvement': (new_patches / new_time) / (old_patches / old_time) if old_time > 0 and new_time > 0 else 0
        }
    }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    print("üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∏ –Ω–æ–≤–æ–≥–æ WSI YOLO Pipeline")
    print("=" * 60)
    
    # –ü—É—Ç–∏ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
    old_results_path = "results/predictions.json"
    new_results_path = "results_improved_full/improved_predictions.json"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    old_results = load_predictions(old_results_path)
    new_results = load_predictions(new_results_path)
    
    if not old_results or not new_results:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        return
    
    old_predictions = old_results.get('predictions', [])
    new_predictions = new_results.get('predictions', [])
    
    print(f"   –°—Ç–∞—Ä—ã–π pipeline: {len(old_predictions)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    print(f"   –ù–æ–≤—ã–π pipeline: {len(new_predictions)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("\nüîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    prediction_comparison = compare_predictions(old_predictions, new_predictions)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    count_comp = prediction_comparison['count_comparison']
    print(f"   –°—Ç–∞—Ä—ã–π: {count_comp['old_count']}")
    print(f"   –ù–æ–≤—ã–π: {count_comp['new_count']}")
    print(f"   –†–∞–∑–Ω–∏—Ü–∞: {count_comp['difference']} ({count_comp['change_percent']:+.1f}%)")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º
    print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    class_comp = prediction_comparison['class_comparison']
    all_classes = set(class_comp['old_classes'].keys()) | set(class_comp['new_classes'].keys())
    
    for class_name in sorted(all_classes):
        old_count = class_comp['old_classes'].get(class_name, 0)
        new_count = class_comp['new_classes'].get(class_name, 0)
        change = new_count - old_count
        change_pct = (change / old_count * 100) if old_count > 0 else 0
        print(f"   {class_name}: {old_count} ‚Üí {new_count} ({change:+.0f}, {change_pct:+.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–∏–≥–æ–Ω–æ–≤
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–∏–≥–æ–Ω–æ–≤:")
    old_poly_stats = prediction_comparison['polygon_analysis']['old_stats']
    new_poly_stats = prediction_comparison['polygon_analysis']['new_stats']
    
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤ –ø–æ–ª–∏–≥–æ–Ω–µ:")
    print(f"     –°—Ç–∞—Ä—ã–π: {old_poly_stats.get('avg_polygon_points', 0):.1f}")
    print(f"     –ù–æ–≤—ã–π: {new_poly_stats.get('avg_polygon_points', 0):.1f}")
    
    if 'avg_area_preserved' in new_poly_stats:
        print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–∏ (–Ω–æ–≤—ã–π): {new_poly_stats['avg_area_preserved']:.1%}")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {new_poly_stats.get('min_area_preserved', 0):.1%}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print(f"\n‚ö° –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    old_perf = old_results.get('performance_stats', {})
    new_perf = new_results.get('performance_stats', {})
    
    if old_perf and new_perf:
        perf_comparison = compare_performance(old_perf, new_perf)
        
        time_comp = perf_comparison['time_comparison']
        print(f"   –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        print(f"     –°—Ç–∞—Ä—ã–π: {time_comp['old_time']:.2f}—Å")
        print(f"     –ù–æ–≤—ã–π: {time_comp['new_time']:.2f}—Å")
        print(f"     –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {time_comp['speedup']:.2f}x")
        print(f"     –≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏: {time_comp['time_saved']:.2f}—Å")
        
        throughput_comp = perf_comparison['throughput_comparison']
        print(f"   –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å:")
        print(f"     –°—Ç–∞—Ä—ã–π: {throughput_comp['old_throughput']:.1f} –ø–∞—Ç—á–µ–π/—Å–µ–∫")
        print(f"     –ù–æ–≤—ã–π: {throughput_comp['new_throughput']:.1f} –ø–∞—Ç—á–µ–π/—Å–µ–∫")
        print(f"     –£–ª—É—á—à–µ–Ω–∏–µ: {throughput_comp['throughput_improvement']:.2f}x")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    if count_comp['change_percent'] > 10:
        print("   ‚ö†Ô∏è  –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é")
    elif count_comp['change_percent'] < -10:
        print("   ‚ö†Ô∏è  –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è")
    else:
        print("   ‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å—Ç–∞–±–∏–ª—å–Ω–æ")
    
    if 'avg_area_preserved' in new_poly_stats and new_poly_stats['avg_area_preserved'] > 0.95:
        print("   ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–∏ –ø–æ–ª–∏–≥–æ–Ω–æ–≤")
    elif 'avg_area_preserved' in new_poly_stats:
        print("   ‚ö†Ô∏è  –°–Ω–∏–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–ª–æ—â–∞–¥–∏ - —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞")
    
    if old_perf and new_perf:
        speedup = perf_comparison['time_comparison']['speedup']
        if speedup > 1.5:
            print(f"   ‚úÖ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ: {speedup:.1f}x")
        elif speedup > 1.1:
            print(f"   ‚úÖ –£–º–µ—Ä–µ–Ω–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ: {speedup:.1f}x")
        else:
            print("   ‚ö†Ô∏è  –ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ - —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")

if __name__ == "__main__":
    main()
